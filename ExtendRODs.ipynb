{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c977cff9-129a-46a6-99da-2528cf5eeebe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T14:58:01.721806200Z",
     "start_time": "2024-02-08T14:58:01.688334500Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk import sent_tokenize\n",
    "import re\n",
    "from ciraprocessor import CiRAProcessor\n",
    "from sys import stderr\n",
    "from cira.src.data.labels import EventLabel\n",
    "import math\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0fd26a1dcd93634",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T14:58:01.732672300Z",
     "start_time": "2024-02-08T14:58:01.723962700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "        self.interval = 60\n",
    "        self.last_write_time = self.start_time\n",
    "        self.elapsed_time = 0\n",
    "        \n",
    "    def log_time(self):\n",
    "        \"\"\"Writes the current time to the timer file if the elapsed time is greater than the interval.\"\"\"\n",
    "        self.elapsed_time = time.time() - self.start_time\n",
    "        if self.elapsed_time - self.last_write_time > self.interval:\n",
    "            with open('timer_4G.txt', 'a') as f:\n",
    "                f.write(f'Elapsed time: {self.elapsed_time} seconds\\n')\n",
    "            self.last_write_time = self.elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ac41e30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T14:58:01.828831500Z",
     "start_time": "2024-02-08T14:58:01.767522700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class RODExtender:\n",
    "    \n",
    "    # Constants\n",
    "    CONDITIONAL_MARKERS = ['if', 'unless', 'upon', 'when', 'whenever', 'as long as', 'so long as']\n",
    "    PMI_THRESHOLD = 3.97\n",
    "    SIMILARITY_THRESHOLD = 0.6 # spacy similarity threshold\n",
    "\n",
    "    def __init__(self, input_file_path, seed_RODs: list[str], output_file_path: str):\n",
    "        classifier_causal_model_path = 'cira/model/cira-classifier.bin'\n",
    "        converter_s2l_model_path = 'cira/model/cira-labeler.ckpt'\n",
    "        self._cira_processor = CiRAProcessor(classifier_causal_model_path, converter_s2l_model_path)\n",
    "        self._seed_RODs = seed_RODs\n",
    "        self._current_RODs = []\n",
    "        with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            self._sentences = sent_tokenize(text)\n",
    "        self._verb_phrases = None\n",
    "        self._current_ROD_id = 0\n",
    "        self._output_file = open(output_file_path, 'w')\n",
    "\n",
    "    '''Returns whether sentence is conditional/causal (used interchangeably for this context)'''\n",
    "\n",
    "    def __get_indices_based_on_condition(self, condition_func, sentences_subset_ids=None):\n",
    "        if sentences_subset_ids is None:\n",
    "            sentences_subset_ids = list(range(len(self._sentences)))\n",
    "        valid_indexes = [index for index in sentences_subset_ids if 0 <= index < len(self._sentences)]\n",
    "        indices = [\n",
    "            index for index in valid_indexes if condition_func(self._sentences[index])\n",
    "        ]\n",
    "        return indices\n",
    "\n",
    "    def __contains_conditional_marker(self, sentence):\n",
    "        # Condition: Check if the sentence contains any of the conditional markers\n",
    "        return any(re.search(fr'\\b{marker}\\b', sentence, re.IGNORECASE) for marker in self.CONDITIONAL_MARKERS)\n",
    "\n",
    "    def __conditional_indices(self, sentences_subset_ids: list[int] = None):\n",
    "        return self.__get_indices_based_on_condition(self.__contains_conditional_marker, sentences_subset_ids)\n",
    "\n",
    "    def __cira_is_causal(self, sentence):\n",
    "        causal, _ = self._cira_processor.cira_classify(sentence)\n",
    "        return causal\n",
    "\n",
    "    def __cira_classified_as_causal_indices(self, sentences_subset_ids: list[int] = None):\n",
    "        return self.__get_indices_based_on_condition(self.__cira_is_causal, sentences_subset_ids)\n",
    "\n",
    "    # noinspection PyPep8Naming\n",
    "    def __contains_ROD(self, sentence):\n",
    "        if len(self._current_RODs[self._current_ROD_id].split()) != 2:\n",
    "            stderr.write(\"ROD is not of length 2.\\n\")\n",
    "            return False\n",
    "        current_ROD_list = self._current_RODs[self._current_ROD_id].split()\n",
    "\n",
    "        # Construct the regular expression pattern for the seed phrase with a maximum of 8 words in between\n",
    "        pattern = re.compile(fr'{re.escape(current_ROD_list[0])} (\\w+ ){{0,8}}{re.escape(current_ROD_list[1])}', re.IGNORECASE)\n",
    "        return re.search(pattern, sentence) is not None\n",
    "\n",
    "    '''Returns whether sentence contains the given seed ROD (verb phrase, 2 words) with 8 or less words in between the words'''\n",
    "\n",
    "    # noinspection PyPep8Naming\n",
    "    def __contain_ROD_indices(self, sentences_subset_ids: list[int] = None):\n",
    "        \"\"\"Returns a list of indices of sentences that contain the given ROD.\"\"\"\n",
    "        if self._current_ROD_id < 0 or self._current_ROD_id > (len(self._current_RODs) - 1):\n",
    "            stderr.write(\"ROD index out of bounds for current_RODs.\")\n",
    "            return\n",
    "        return self.__get_indices_based_on_condition(self.__contains_ROD, sentences_subset_ids)\n",
    "\n",
    "    def __find_conditions_for_sentence(self, sentence) -> list[str]:\n",
    "        \"\"\"Returns a list of conditions in the given sentence.\"\"\"\n",
    "        # causal, _ = self._cira_processor.cira_classify(sentence)\n",
    "        # if not causal:\n",
    "        #     return []\n",
    "        conditions = []\n",
    "        labels = self._cira_processor.cira_label(sentence)\n",
    "        for label in labels:\n",
    "            if (isinstance(label, EventLabel)) and label.is_cause():\n",
    "                sorted_children = sorted(label.children, key=lambda x: x.begin)\n",
    "                for i, sublabel in enumerate(sorted_children):\n",
    "                    if sublabel.name == \"Condition\":\n",
    "                        condition = sentence[sublabel.begin:sublabel.end]\n",
    "                        # Check if the condition is preceded by a negation\n",
    "                        if i > 0 and sorted_children[i - 1].name == \"Negation\":\n",
    "                            condition = \"not \" + condition\n",
    "                        conditions.append(condition)\n",
    "        return conditions\n",
    "    \n",
    "    def __conditions_overlap(self, sentence1, sentence2, nlp):\n",
    "        '''Returns whether the sentences' conditions overlap (at least one common condition) using spacy similarity'''\n",
    "        conditions1 = self.__find_conditions_for_sentence(sentence1)\n",
    "        conditions2 = self.__find_conditions_for_sentence(sentence2)\n",
    "        for condition1 in conditions1:\n",
    "            for condition2 in conditions2:\n",
    "                similarity = nlp(condition1).similarity(nlp(condition2))\n",
    "                if similarity > self.SIMILARITY_THRESHOLD:\n",
    "                    return True\n",
    "        return False \n",
    "    \n",
    "    \n",
    "    def __find_subjects(self, sentence) -> list[str]:\n",
    "        '''Returns a list of subjects in the given sentence.'''\n",
    "        nlp = spacy.load(\"en_core_web_lg\")\n",
    "        doc = nlp(sentence)\n",
    "        subjects = []\n",
    "        for token in doc:\n",
    "            if token.dep_ == \"nsubj\":\n",
    "                subjects.append(token.text)\n",
    "        return subjects\n",
    "    \n",
    "    \n",
    "    def __subject_overlap(self, sentence1, sentence2, nlp) -> bool:\n",
    "        '''Returns whether the sentences'/conditions' nominal subjects overlap (at least one common subject) using spacy similarity and returns the overlapping subject from sentence2'''\n",
    "        subjects1 = self.__find_subjects(sentence1)\n",
    "        subjects2 = self.__find_subjects(sentence2)\n",
    "        for subject1 in subjects1:\n",
    "            for subject2 in subjects2:\n",
    "                if nlp(subject1).similarity(nlp(subject2)) > self.SIMILARITY_THRESHOLD:\n",
    "                    return True\n",
    "        return False\n",
    "                    \n",
    "    def __extract_verb_phrases(self, sentence):\n",
    "        '''Extracts verb phrases (verb + object) from the given sentence using dependency parsing, not suitable for complicated sentences.'''\n",
    "        nlp = spacy.load(\"en_core_web_lg\")\n",
    "        doc = nlp(sentence)\n",
    "        verb_phrases = []\n",
    "        for token in doc:\n",
    "            if token.pos_ == \"VERB\":\n",
    "                for child in token.children:\n",
    "                    if child.dep_ in {\"dobj\", \"attr\"}:\n",
    "                        verb_phrase = ' '.join([token.lemma_, child.lemma_])\n",
    "                        verb_phrases.append(verb_phrase)\n",
    "                    elif child.dep_ == \"ccomp\":\n",
    "                        for grandchild in child.children:\n",
    "                            if grandchild.dep_ == \"nsubj\":\n",
    "                                verb_phrase = ' '.join([token.lemma_, grandchild.lemma_])\n",
    "                                verb_phrases.append(verb_phrase)        \n",
    "        return verb_phrases\n",
    "    \n",
    "    def __total_count_verb_phrases(self):\n",
    "        '''Returns the total number of verb phrases in all sentences.'''\n",
    "        return sum(len(verb_phrases) for verb_phrases in self._verb_phrases)\n",
    "        \n",
    "    def __identify_cooc_verb_phrases(self, ROD_sentence, neighbor_sentence, ROD):\n",
    "        '''Identify co-occuring verb phrases with the same subject and condition and a PMI above the threshold. Returns a list of co-occuring verb phrases for the two given sentences.'''\n",
    "        nlp = spacy.load('en_core_web_lg')\n",
    "        subject_overlap = self.__subject_overlap(ROD_sentence, neighbor_sentence, nlp)\n",
    "        if subject_overlap is False:\n",
    "            return []\n",
    "        try: \n",
    "            conditions_overlap = self.__conditions_overlap(ROD_sentence, neighbor_sentence, nlp)\n",
    "        except AttributeError:\n",
    "            # the labelingconverter.py from cira may throw an AttributeError (for unknown reasons), so we catch it here\n",
    "            return []\n",
    "        # if the subject and the conditions overlap, check for co-occurring verb phrases\n",
    "        if conditions_overlap:\n",
    "            cooc_verb_phrases = []\n",
    "            new_verb_phrases = self._verb_phrases[self._sentences.index(neighbor_sentence)]\n",
    "            for verb_phrase in new_verb_phrases:\n",
    "                if verb_phrase == ROD:\n",
    "                    continue\n",
    "                # use pmi to calculate the similarity between the new verb phrase and ROD\n",
    "                pmi = self.__pmi(ROD, verb_phrase)\n",
    "                self.__write_to_file((\"PMI of \\\"\" + verb_phrase + \"\\\" and \\\"\" + ROD + \"\\\": \" + str(pmi)))                    \n",
    "                if pmi > self.PMI_THRESHOLD:\n",
    "                    self.__write_to_file(verb_phrase + \" is a co-occurring verb phrase with \" + ROD + \" in the sentence: \" + neighbor_sentence)\n",
    "                    cooc_verb_phrases.append(verb_phrase)\n",
    "            return cooc_verb_phrases\n",
    "        else: return []\n",
    "        \n",
    "    \n",
    "    def __pmi(self, ROD, verb_phrase2):\n",
    "        \"\"\"Calculates the pointwise mutual information between the given verb phrases: PMI(x, y) = log2(P(x, y) / (P(x) * P(y)))\n",
    "        \"\"\"\n",
    "        total_count = self.__total_count_verb_phrases()\n",
    "        count_xy = self.__cooc_phrase_count(ROD, verb_phrase2)\n",
    "        if count_xy == 0:\n",
    "            return 0\n",
    "        count_x = self.__count_phrase(ROD)\n",
    "        count_y = self.__count_phrase(verb_phrase2)\n",
    "        pmi = math.log((count_xy / total_count) / ((count_x / total_count) * (count_y / total_count)))\n",
    "        return pmi\n",
    "    \n",
    "    def __count_phrase(self, phrase):\n",
    "        \"\"\"Calculates the number of occurences of the given phrase in the document.\"\"\"\n",
    "        return sum(sublist.count(phrase) for sublist in self._verb_phrases)\n",
    "    \n",
    "\n",
    "    def __cooc_phrase_count(self, ROD, phrase2):\n",
    "        \"\"\"Calculates the number of co-occurences an ROD and a verb phrase in the document.\n",
    "        They are considered to co-occur if they are present in the same sentence or in neighboring sentences.\"\"\"\n",
    "        cooc_verb_phrases_count = 0\n",
    "        # or self._sentences[phrase].contains(ROD)\n",
    "        # add ROD to phrase list if it is present in the sentence but did not get extracted as a verb phrase\n",
    "        self._verb_phrases = [sublist + [ROD] if ROD not in sublist and self.__contains_ROD(self._sentences[i]) else sublist for i, sublist in enumerate(self._verb_phrases)]\n",
    "        verb_phrases = [[phrase for phrase in sublist if phrase == ROD or phrase == phrase2 or self.__contains_ROD(self._sentences[ind])] for ind, sublist in enumerate(self._verb_phrases)]\n",
    "        for index in range(len(verb_phrases)):\n",
    "            if ROD in verb_phrases[index] and phrase2 in verb_phrases[index]:\n",
    "                cooc_verb_phrases_count += 1\n",
    "                # remove the phrases from the list to avoid double counting\n",
    "                verb_phrases[index].remove(ROD)\n",
    "                verb_phrases[index].remove(phrase2)\n",
    "            if index < len(verb_phrases) - 1:\n",
    "                # merge the list of verb phrases from the next sentence with the current sentence's verb phrases\n",
    "                merged_verb_phr = verb_phrases[index] + verb_phrases[index+1]\n",
    "                if ROD in merged_verb_phr and phrase2 in merged_verb_phr:\n",
    "                    cooc_verb_phrases_count += 1\n",
    "                    # after counting the co-oc, remove the phrases from the list to avoid double counting\n",
    "                    try:\n",
    "                        verb_phrases[index].remove(ROD)\n",
    "                    except ValueError:\n",
    "                        verb_phrases[index+1].remove(ROD)\n",
    "                    try:\n",
    "                        verb_phrases[index].remove(phrase2)\n",
    "                    except ValueError:\n",
    "                        verb_phrases[index+1].remove(phrase2)\n",
    "        return cooc_verb_phrases_count\n",
    "    \n",
    "    def __search_neighbors(self, _indices, ROD, timer):\n",
    "        \"\"\"Searches the neighboring sentences of the given index for co-occurring verb phrases with the current ROD.\"\"\"\n",
    "        cooc_verb_phrases = []\n",
    "        for index in _indices:\n",
    "            # log time here as this loop is not too time-consuming but also not too fast\n",
    "            timer.log_time()\n",
    "            if index > 0: # and index - 1 not in _indices:\n",
    "                    cooc_verb_phrases += self.__identify_cooc_verb_phrases(self._sentences[index], self._sentences[index-1], ROD)\n",
    "            if index < len(self._sentences) - 1: # and index + 1 not in _indices:\n",
    "                    cooc_verb_phrases += self.__identify_cooc_verb_phrases(self._sentences[index], self._sentences[index+1], ROD)\n",
    "        self.__write_to_file(\"self.__search_neighbors returns co-occurring verb phrases with \" + ROD + \":\")\n",
    "        return cooc_verb_phrases\n",
    "    \n",
    "    def __write_to_file(self, message):\n",
    "        \"\"\"Writes the given message to the output file.\"\"\"\n",
    "        try:\n",
    "            self._output_file.write(message + \"\\n\")\n",
    "            self._output_file.flush()\n",
    "            os.fsync(self._output_file.fileno())\n",
    "        except IOError as e:\n",
    "            print(f\"An IOError occurred: {e}\")\n",
    "        finally:\n",
    "            if self._output_file in locals():  # Check if output_file exists\n",
    "                self._output_file.close()\n",
    "                        \n",
    "        \n",
    "    def extend(self):\n",
    "        \"\"\"Extends the seed RODs to new RODs based on the given sentences.\n",
    "            Returns a list of lists of extended RODs for each seed ROD.\"\"\"\n",
    "        timer = Timer()\n",
    "        try:           \n",
    "            self.__write_to_file(\"Filtering indices based on conditional markers...\")\n",
    "            indices = self.__conditional_indices()\n",
    "            self.__write_to_file(\"Number of conditional sentences: \" + str(len(indices)))\n",
    "            indices = self.__cira_classified_as_causal_indices(indices)\n",
    "            self.__write_to_file(\"Number of sentences classified by CiRA as causal sentences: \" + str(len(indices)))\n",
    "            self.__write_to_file(\"Extracting all verb phrases from sentences...\")\n",
    "            self._verb_phrases = [self.__extract_verb_phrases(sentence) for sentence in self._sentences]\n",
    "    \n",
    "            # check each seed_ROD separately\n",
    "            final_RODs = [[] for _ in self._seed_RODs]\n",
    "            \n",
    "            # for each seed ROD, find all RODs that are extended from it\n",
    "            for seed in self._seed_RODs:\n",
    "                # copy indices bc they are needed for each seed\n",
    "                _indices = indices[:]\n",
    "                checked_RODs = []\n",
    "                # start with the seed ROD\n",
    "                self._current_RODs = [seed]\n",
    "                # while there are still RODs to check\n",
    "                while self._current_RODs:\n",
    "                    self.__write_to_file(\"Current RODs to check:\")\n",
    "                    self.__write_to_file(str(self._current_RODs))\n",
    "                    new_RODs = []\n",
    "                    for ROD_id, ROD in enumerate(self._current_RODs):\n",
    "                        self.__write_to_file(\"Checking ROD: \" + ROD)\n",
    "                        # set the current ROD id as self.__contain_ROD_indices needs it\n",
    "                        self._current_ROD_id = ROD_id\n",
    "                        # get the indices of the sentences that contain the current ROD\n",
    "                        _indices = self.__contain_ROD_indices(_indices)\n",
    "                        # for each sentence that contains the ROD, check if there are co-occurring verb phrases\n",
    "                        cooc_verb_phrases = self.__search_neighbors(_indices, ROD, timer)\n",
    "                        # add the new RODs to the list of RODs to check\n",
    "                        new_RODs += [new_ROD for new_ROD in cooc_verb_phrases if new_ROD not in checked_RODs and new_ROD not in new_RODs]\n",
    "                        # remove the current ROD from the list of RODs to check\n",
    "                        # self._current_RODs.remove(ROD)\n",
    "                        # add the current ROD to the list of checked RODs (and from this later to the list of final RODs)\n",
    "                        checked_RODs.append(ROD)\n",
    "                    if new_RODs:\n",
    "                        self.__write_to_file(\"New RODs to check:\")\n",
    "                        self.__write_to_file(str(new_RODs))\n",
    "                    else: \n",
    "                        self.__write_to_file(\"No new RODs to check.\")\n",
    "                    self._current_RODs = new_RODs\n",
    "                self._current_RODs = []\n",
    "                # save the checked RODs to the list corresponding to the seed ROD, remove duplicates\n",
    "                final_RODs[self._seed_RODs.index(seed)] = list(set(checked_RODs))\n",
    "                self.__write_to_file(\"RODs extended from seed \" + seed + \":\")\n",
    "                self.__write_to_file(str(checked_RODs))\n",
    "            \n",
    "            # write results to file\n",
    "            self.__write_to_file(\"FINAL RESULTS:\")\n",
    "            for id, seed_ROD in enumerate(self._seed_RODs):\n",
    "                self.__write_to_file(\"RODs extended from \" + seed_ROD + \":\")\n",
    "                self.__write_to_file(str(final_RODs[id]))\n",
    "            # write final elapsed time to output file\n",
    "            self.__write_to_file(f'The program finished. Final elapsed time: {timer.elapsed_time} seconds\\n')\n",
    "            self._output_file.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            # If an error occurs, write the current time to the timer file, close the output file and raise the error\n",
    "            with open('timer_4G.txt', 'a') as f:\n",
    "                f.write(f'Elapsed time at crash: {timer.elapsed_time} seconds\\n')\n",
    "            self._output_file.close()\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            raise e\n",
    "    \n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b641cc9-34fe-4122-af05-4cc851cbc3e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T14:58:01.828831500Z",
     "start_time": "2024-02-08T14:58:01.828831500Z"
    }
   },
   "outputs": [],
   "source": [
    "classifier_model_path = 'cira/model/cira-classifier.bin'\n",
    "converter_model_path = 'cira/model/cira-labeler.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baa5b0091556c2a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T14:59:50.784987900Z",
     "start_time": "2024-02-08T14:58:01.828831500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "seed_RODs = [\"abort procedure\", \"consider USIM\"]\n",
    "# input_path = \"demo.txt\"\n",
    "# output_path = \"extended_demo.txt\"\n",
    "# input_path = \"processed_5G_NAS.txt\"\n",
    "# output_path = \"5G_NAS_extended_RODs_final_final.txt\"\n",
    "input_path = \"processed_LTE_NAS.txt\"\n",
    "output_path = \"LTE_NAS_extended_RODs_final.txt\"\n",
    "extender = RODExtender(input_path, seed_RODs, output_path)\n",
    "extended_RODs = extender.extend()    \n",
    "# bert-base-cased weights warning is expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cbe5de19ab785df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T14:59:50.829706Z",
     "start_time": "2024-02-08T14:59:50.783902300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
