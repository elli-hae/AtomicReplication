{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "Modified from [Sentence-Entailment repository](https://colab.research.google.com/github/dh1105/Sentence-Entailment/blob/main/Sentence_Entailment_BERT.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T17:39:43.188188700Z",
     "start_time": "2024-02-03T17:39:42.964188400Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uqcn3_lQ5Hhr",
    "outputId": "3ee374ba-39ca-472e-d392-bdb4f6e4638e"
   },
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "#!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T17:39:44.698029100Z",
     "start_time": "2024-02-03T17:39:43.188188700Z"
    },
    "id": "JQ4M8Znd4_ep"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "# import torch_xla\n",
    "# import torch_xla.core.xla_model as xm\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, SequentialSampler, RandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T17:39:44.741963800Z",
     "start_time": "2024-02-03T17:39:44.740966300Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WkpfPDS1Z8a8",
    "outputId": "3219bc62-f97b-499f-a28a-0379e1560f71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T17:39:46.745640300Z",
     "start_time": "2024-02-03T17:39:44.740966300Z"
    },
    "id": "9vYZA1zq4_e7"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet('data/train-00000-of-00001.parquet')[:10]\n",
    "val_df   = pd.read_parquet('data/validation_mismatched-00000-of-00001.parquet')[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T17:39:46.915678500Z",
     "start_time": "2024-02-03T17:39:46.901636800Z"
    },
    "id": "3NgPYXBg4_e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre', 'label']\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df.dropna()\n",
    "val_df = val_df.dropna()\n",
    "print(train_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T17:39:46.916709400Z",
     "start_time": "2024-02-03T17:39:46.912662300Z"
    },
    "id": "UMTQb1iMF54W"
   },
   "outputs": [],
   "source": [
    "train_df['premise'] = train_df['premise'].astype(str)\n",
    "train_df['hypothesis'] = train_df['hypothesis'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T17:39:46.973668100Z",
     "start_time": "2024-02-03T17:39:46.936640200Z"
    },
    "id": "GMzPNmtpF8yg"
   },
   "outputs": [],
   "source": [
    "val_df['premise'] = val_df['premise'].astype(str)\n",
    "val_df['hypothesis'] = val_df['hypothesis'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T17:39:47.010636900Z",
     "start_time": "2024-02-03T17:39:46.955650Z"
    },
    "id": "diS3wCm14_e9"
   },
   "outputs": [],
   "source": [
    "train_df = train_df[(train_df['premise'].str.split().str.len() > 0) & (train_df['hypothesis'].str.split().str.len() > 0)]\n",
    "val_df = val_df[(val_df['premise'].str.split().str.len() > 0) & (val_df['hypothesis'].str.split().str.len() > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T17:39:47.012637100Z",
     "start_time": "2024-02-03T17:39:47.003637500Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "id": "WinUrXE04_e9",
    "outputId": "974ce2c2-04b4-40e2-f33d-ee172eb6c49e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>promptID</th>\n",
       "      <th>pairID</th>\n",
       "      <th>premise</th>\n",
       "      <th>premise_binary_parse</th>\n",
       "      <th>premise_parse</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>hypothesis_binary_parse</th>\n",
       "      <th>hypothesis_parse</th>\n",
       "      <th>genre</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31193</td>\n",
       "      <td>31193n</td>\n",
       "      <td>Conceptually cream skimming has two basic dime...</td>\n",
       "      <td>( ( Conceptually ( cream skimming ) ) ( ( has ...</td>\n",
       "      <td>(ROOT (S (NP (JJ Conceptually) (NN cream) (NN ...</td>\n",
       "      <td>Product and geography are what make cream skim...</td>\n",
       "      <td>( ( ( Product and ) geography ) ( ( are ( what...</td>\n",
       "      <td>(ROOT (S (NP (NN Product) (CC and) (NN geograp...</td>\n",
       "      <td>government</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101457</td>\n",
       "      <td>101457e</td>\n",
       "      <td>you know during the season and i guess at at y...</td>\n",
       "      <td>( you ( ( know ( during ( ( ( the season ) and...</td>\n",
       "      <td>(ROOT (S (NP (PRP you)) (VP (VBP know) (PP (IN...</td>\n",
       "      <td>You lose the things to the following level if ...</td>\n",
       "      <td>( You ( ( ( ( lose ( the things ) ) ( to ( the...</td>\n",
       "      <td>(ROOT (S (NP (PRP You)) (VP (VBP lose) (NP (DT...</td>\n",
       "      <td>telephone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>134793</td>\n",
       "      <td>134793e</td>\n",
       "      <td>One of our number will carry out your instruct...</td>\n",
       "      <td>( ( One ( of ( our number ) ) ) ( ( will ( ( (...</td>\n",
       "      <td>(ROOT (S (NP (NP (CD One)) (PP (IN of) (NP (PR...</td>\n",
       "      <td>A member of my team will execute your orders w...</td>\n",
       "      <td>( ( ( A member ) ( of ( my team ) ) ) ( ( will...</td>\n",
       "      <td>(ROOT (S (NP (NP (DT A) (NN member)) (PP (IN o...</td>\n",
       "      <td>fiction</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37397</td>\n",
       "      <td>37397e</td>\n",
       "      <td>How do you know? All this is their information...</td>\n",
       "      <td>( ( How ( ( ( do you ) know ) ? ) ) ( ( All th...</td>\n",
       "      <td>(ROOT (S (SBARQ (WHADVP (WRB How)) (SQ (VBP do...</td>\n",
       "      <td>This information belongs to them.</td>\n",
       "      <td>( ( This information ) ( ( belongs ( to them )...</td>\n",
       "      <td>(ROOT (S (NP (DT This) (NN information)) (VP (...</td>\n",
       "      <td>fiction</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50563</td>\n",
       "      <td>50563n</td>\n",
       "      <td>yeah i tell you what though if you go price so...</td>\n",
       "      <td>( yeah ( i ( ( tell you ) ( what ( ( though ( ...</td>\n",
       "      <td>(ROOT (S (VP (VB yeah) (S (NP (FW i)) (VP (VB ...</td>\n",
       "      <td>The tennis shoes have a range of prices.</td>\n",
       "      <td>( ( The ( tennis shoes ) ) ( ( have ( ( a rang...</td>\n",
       "      <td>(ROOT (S (NP (DT The) (NN tennis) (NNS shoes))...</td>\n",
       "      <td>telephone</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>144385</td>\n",
       "      <td>144385c</td>\n",
       "      <td>Click More Links (on the right-hand side under...</td>\n",
       "      <td>( Click ( ( ( More Links ) ( -LRB- ( ( on ( ( ...</td>\n",
       "      <td>(ROOT (S (VP (VB Click) (ADJP (ADJP (ADJP (RBR...</td>\n",
       "      <td>There are no links to click under Miscellaneous.</td>\n",
       "      <td>( There ( ( are ( no ( links ( to ( click ( un...</td>\n",
       "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (DT...</td>\n",
       "      <td>slate</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>14868</td>\n",
       "      <td>14868n</td>\n",
       "      <td>and so i started watching it and all of a sudd...</td>\n",
       "      <td>( and ( so ( ( ( i ( ( started ( watching ( ( ...</td>\n",
       "      <td>(ROOT (PRN (CC and) (ADJP (RB so) (SBAR (S (S ...</td>\n",
       "      <td>I wouldn't have started watching it if I'd known.</td>\n",
       "      <td>( I ( ( ( would n't ) ( have ( started ( ( wat...</td>\n",
       "      <td>(ROOT (S (NP (PRP I)) (VP (MD would) (RB n't) ...</td>\n",
       "      <td>telephone</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>133436</td>\n",
       "      <td>133436e</td>\n",
       "      <td>no oh no oh well take care</td>\n",
       "      <td>( ( no oh ) ( ( ( ( no oh ) well ) take ) care...</td>\n",
       "      <td>(ROOT (SINV (FRAG (INTJ (UH no) (UH oh))) (VP ...</td>\n",
       "      <td>Bye for now.</td>\n",
       "      <td>( ( Bye ( for now ) ) . )</td>\n",
       "      <td>(ROOT (S (VP (VB Bye) (PP (IN for) (NP (RB now...</td>\n",
       "      <td>telephone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>26025</td>\n",
       "      <td>26025c</td>\n",
       "      <td>'Hello, Ben.'</td>\n",
       "      <td>( ` ( Hello ( , ( ( Ben . ) ' ) ) ) )</td>\n",
       "      <td>(ROOT (S (`` `) (ADVP (RB Hello)) (, ,) (VP (V...</td>\n",
       "      <td>I ignored Ben</td>\n",
       "      <td>( I ( ignored Ben ) )</td>\n",
       "      <td>(ROOT (S (NP (PRP I)) (VP (VBD ignored) (NP (N...</td>\n",
       "      <td>fiction</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>24696</td>\n",
       "      <td>24696e</td>\n",
       "      <td>how can you prove it</td>\n",
       "      <td>( how ( ( can you ) ( prove it ) ) )</td>\n",
       "      <td>(ROOT (SBARQ (WHADVP (WRB how)) (SQ (MD can) (...</td>\n",
       "      <td>Can you tell me how to prove it?</td>\n",
       "      <td>( ( ( Can you ) ( ( tell me ) ( how ( to ( pro...</td>\n",
       "      <td>(ROOT (SQ (MD Can) (NP (PRP you)) (VP (VB tell...</td>\n",
       "      <td>telephone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    promptID   pairID                                            premise  \\\n",
       "0      31193   31193n  Conceptually cream skimming has two basic dime...   \n",
       "1     101457  101457e  you know during the season and i guess at at y...   \n",
       "2     134793  134793e  One of our number will carry out your instruct...   \n",
       "3      37397   37397e  How do you know? All this is their information...   \n",
       "4      50563   50563n  yeah i tell you what though if you go price so...   \n",
       "..       ...      ...                                                ...   \n",
       "95    144385  144385c  Click More Links (on the right-hand side under...   \n",
       "96     14868   14868n  and so i started watching it and all of a sudd...   \n",
       "97    133436  133436e                         no oh no oh well take care   \n",
       "98     26025   26025c                                      'Hello, Ben.'   \n",
       "99     24696   24696e                               how can you prove it   \n",
       "\n",
       "                                 premise_binary_parse  \\\n",
       "0   ( ( Conceptually ( cream skimming ) ) ( ( has ...   \n",
       "1   ( you ( ( know ( during ( ( ( the season ) and...   \n",
       "2   ( ( One ( of ( our number ) ) ) ( ( will ( ( (...   \n",
       "3   ( ( How ( ( ( do you ) know ) ? ) ) ( ( All th...   \n",
       "4   ( yeah ( i ( ( tell you ) ( what ( ( though ( ...   \n",
       "..                                                ...   \n",
       "95  ( Click ( ( ( More Links ) ( -LRB- ( ( on ( ( ...   \n",
       "96  ( and ( so ( ( ( i ( ( started ( watching ( ( ...   \n",
       "97  ( ( no oh ) ( ( ( ( no oh ) well ) take ) care...   \n",
       "98              ( ` ( Hello ( , ( ( Ben . ) ' ) ) ) )   \n",
       "99               ( how ( ( can you ) ( prove it ) ) )   \n",
       "\n",
       "                                        premise_parse  \\\n",
       "0   (ROOT (S (NP (JJ Conceptually) (NN cream) (NN ...   \n",
       "1   (ROOT (S (NP (PRP you)) (VP (VBP know) (PP (IN...   \n",
       "2   (ROOT (S (NP (NP (CD One)) (PP (IN of) (NP (PR...   \n",
       "3   (ROOT (S (SBARQ (WHADVP (WRB How)) (SQ (VBP do...   \n",
       "4   (ROOT (S (VP (VB yeah) (S (NP (FW i)) (VP (VB ...   \n",
       "..                                                ...   \n",
       "95  (ROOT (S (VP (VB Click) (ADJP (ADJP (ADJP (RBR...   \n",
       "96  (ROOT (PRN (CC and) (ADJP (RB so) (SBAR (S (S ...   \n",
       "97  (ROOT (SINV (FRAG (INTJ (UH no) (UH oh))) (VP ...   \n",
       "98  (ROOT (S (`` `) (ADVP (RB Hello)) (, ,) (VP (V...   \n",
       "99  (ROOT (SBARQ (WHADVP (WRB how)) (SQ (MD can) (...   \n",
       "\n",
       "                                           hypothesis  \\\n",
       "0   Product and geography are what make cream skim...   \n",
       "1   You lose the things to the following level if ...   \n",
       "2   A member of my team will execute your orders w...   \n",
       "3                   This information belongs to them.   \n",
       "4            The tennis shoes have a range of prices.   \n",
       "..                                                ...   \n",
       "95   There are no links to click under Miscellaneous.   \n",
       "96  I wouldn't have started watching it if I'd known.   \n",
       "97                                       Bye for now.   \n",
       "98                                      I ignored Ben   \n",
       "99                   Can you tell me how to prove it?   \n",
       "\n",
       "                              hypothesis_binary_parse  \\\n",
       "0   ( ( ( Product and ) geography ) ( ( are ( what...   \n",
       "1   ( You ( ( ( ( lose ( the things ) ) ( to ( the...   \n",
       "2   ( ( ( A member ) ( of ( my team ) ) ) ( ( will...   \n",
       "3   ( ( This information ) ( ( belongs ( to them )...   \n",
       "4   ( ( The ( tennis shoes ) ) ( ( have ( ( a rang...   \n",
       "..                                                ...   \n",
       "95  ( There ( ( are ( no ( links ( to ( click ( un...   \n",
       "96  ( I ( ( ( would n't ) ( have ( started ( ( wat...   \n",
       "97                          ( ( Bye ( for now ) ) . )   \n",
       "98                              ( I ( ignored Ben ) )   \n",
       "99  ( ( ( Can you ) ( ( tell me ) ( how ( to ( pro...   \n",
       "\n",
       "                                     hypothesis_parse       genre  label  \n",
       "0   (ROOT (S (NP (NN Product) (CC and) (NN geograp...  government      1  \n",
       "1   (ROOT (S (NP (PRP You)) (VP (VBP lose) (NP (DT...   telephone      0  \n",
       "2   (ROOT (S (NP (NP (DT A) (NN member)) (PP (IN o...     fiction      0  \n",
       "3   (ROOT (S (NP (DT This) (NN information)) (VP (...     fiction      0  \n",
       "4   (ROOT (S (NP (DT The) (NN tennis) (NNS shoes))...   telephone      1  \n",
       "..                                                ...         ...    ...  \n",
       "95  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (DT...       slate      2  \n",
       "96  (ROOT (S (NP (PRP I)) (VP (MD would) (RB n't) ...   telephone      1  \n",
       "97  (ROOT (S (VP (VB Bye) (PP (IN for) (NP (RB now...   telephone      0  \n",
       "98  (ROOT (S (NP (PRP I)) (VP (VBD ignored) (NP (N...     fiction      2  \n",
       "99  (ROOT (SQ (MD Can) (NP (PRP you)) (VP (VB tell...   telephone      0  \n",
       "\n",
       "[100 rows x 10 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T17:39:47.080661100Z",
     "start_time": "2024-02-03T17:39:47.044660400Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "id": "6e4dxTks4_e-",
    "outputId": "d6e58040-d442-48e3-9f44-563dca5b9031"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>promptID</th>\n",
       "      <th>pairID</th>\n",
       "      <th>premise</th>\n",
       "      <th>premise_binary_parse</th>\n",
       "      <th>premise_parse</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>hypothesis_binary_parse</th>\n",
       "      <th>hypothesis_parse</th>\n",
       "      <th>genre</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75290</td>\n",
       "      <td>75290c</td>\n",
       "      <td>Your contribution helped make it possible for ...</td>\n",
       "      <td>( ( Your contribution ) ( ( helped ( make ( it...</td>\n",
       "      <td>(ROOT (S (NP (PRP$ Your) (NN contribution)) (V...</td>\n",
       "      <td>Your contributions were of no help with our st...</td>\n",
       "      <td>( ( Your contributions ) ( ( were ( of ( ( no ...</td>\n",
       "      <td>(ROOT (S (NP (PRP$ Your) (NNS contributions)) ...</td>\n",
       "      <td>letters</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>133794</td>\n",
       "      <td>133794c</td>\n",
       "      <td>The answer has nothing to do with their cause,...</td>\n",
       "      <td>( ( ( ( ( ( The answer ) ( ( ( ( has nothing )...</td>\n",
       "      <td>(ROOT (S (S (NP (DT The) (NN answer)) (VP (VBZ...</td>\n",
       "      <td>Dictionaries are indeed exercises in bi-unique...</td>\n",
       "      <td>( Dictionaries ( ( ( are indeed ) ( exercises ...</td>\n",
       "      <td>(ROOT (S (NP (NNS Dictionaries)) (VP (VBP are)...</td>\n",
       "      <td>verbatim</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3628</td>\n",
       "      <td>3628c</td>\n",
       "      <td>We serve a classic Tuscan meal that includes ...</td>\n",
       "      <td>( We ( ( serve ( ( a ( classic ( Tuscan meal )...</td>\n",
       "      <td>(ROOT (S (NP (PRP We)) (VP (VBP serve) (NP (NP...</td>\n",
       "      <td>We serve a meal of Florentine terrine.</td>\n",
       "      <td>( We ( ( serve ( ( a meal ) ( of ( Florentine ...</td>\n",
       "      <td>(ROOT (S (NP (PRP We)) (VP (VBP serve) (NP (NP...</td>\n",
       "      <td>verbatim</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89411</td>\n",
       "      <td>89411c</td>\n",
       "      <td>A few months ago, Carl Newton and I wrote a le...</td>\n",
       "      <td>( ( ( A ( few months ) ) ago ) ( , ( ( ( ( Car...</td>\n",
       "      <td>(ROOT (S (ADVP (NP (DT A) (JJ few) (NNS months...</td>\n",
       "      <td>Carl Newton and I have never had any other pre...</td>\n",
       "      <td>( ( ( ( Carl Newton ) and ) I ) ( ( ( have nev...</td>\n",
       "      <td>(ROOT (S (NP (NP (NNP Carl) (NNP Newton)) (CC ...</td>\n",
       "      <td>letters</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>136158</td>\n",
       "      <td>136158e</td>\n",
       "      <td>I was on this earth you know, I've lived on th...</td>\n",
       "      <td>( I ( ( was ( on ( ( this earth ) ( you ( know...</td>\n",
       "      <td>(ROOT (S (NP (PRP I)) (VP (VBD was) (PP (IN on...</td>\n",
       "      <td>I don't yet know the reason why I have lived o...</td>\n",
       "      <td>( I ( ( ( ( do n't ) yet ) ( ( know ( the reas...</td>\n",
       "      <td>(ROOT (S (NP (PRP I)) (VP (VBP do) (RB n't) (A...</td>\n",
       "      <td>facetoface</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>75857</td>\n",
       "      <td>75857c</td>\n",
       "      <td>Recently we read that how children turn out is...</td>\n",
       "      <td>( ( ( ( Recently ( we ( read ( that ( ( how ( ...</td>\n",
       "      <td>(ROOT (S (S (ADVP (RB Recently)) (NP (PRP we))...</td>\n",
       "      <td>How children turn out is primarily determined ...</td>\n",
       "      <td>( ( How ( children ( turn out ) ) ) ( ( ( is p...</td>\n",
       "      <td>(ROOT (S (SBAR (WHADVP (WRB How)) (S (NP (NNS ...</td>\n",
       "      <td>oup</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>101420</td>\n",
       "      <td>101420n</td>\n",
       "      <td>And I said to some of my cousins that I had on...</td>\n",
       "      <td>( And ( I ( ( ( said ( to ( some ( of ( my cou...</td>\n",
       "      <td>(ROOT (S (CC And) (NP (PRP I)) (VP (VBD said) ...</td>\n",
       "      <td>The last time I saw my nieces were at grandfat...</td>\n",
       "      <td>( ( ( The ( last time ) ) ( I ( saw ( my niece...</td>\n",
       "      <td>(ROOT (S (NP (NP (DT The) (JJ last) (NN time))...</td>\n",
       "      <td>facetoface</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>131793</td>\n",
       "      <td>131793e</td>\n",
       "      <td>He views either of them as the instrument, the...</td>\n",
       "      <td>( He ( ( ( views ( either ( of them ) ) ) ( as...</td>\n",
       "      <td>(ROOT (S (NP (PRP He)) (VP (VBZ views) (NP (NP...</td>\n",
       "      <td>He cannot pass the klngship onto someone else.</td>\n",
       "      <td>( He ( ( ( can not ) ( ( pass ( the klngship )...</td>\n",
       "      <td>(ROOT (S (NP (PRP He)) (VP (MD can) (RB not) (...</td>\n",
       "      <td>verbatim</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>121697</td>\n",
       "      <td>121697e</td>\n",
       "      <td>Something to that effect.</td>\n",
       "      <td>( ( Something ( to ( that effect ) ) ) . )</td>\n",
       "      <td>(ROOT (NP (NP (NN Something)) (PP (TO to) (NP ...</td>\n",
       "      <td>That's approximately what it means.</td>\n",
       "      <td>( That ( ( 's ( approximately ( what ( it mean...</td>\n",
       "      <td>(ROOT (S (NP (DT That)) (VP (VBZ 's) (ADJP (RB...</td>\n",
       "      <td>facetoface</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>3901</td>\n",
       "      <td>3901c</td>\n",
       "      <td>I do not know how he would rate it, but I must...</td>\n",
       "      <td>( ( ( ( ( I ( ( do not ) ( know ( how ( he ( w...</td>\n",
       "      <td>(ROOT (S (S (NP (PRP I)) (VP (VBP do) (RB not)...</td>\n",
       "      <td>I'm sure he'd consider Chambers to be better, ...</td>\n",
       "      <td>( ( ( ( ( I ( 'm ( sure ( he ( 'd ( consider (...</td>\n",
       "      <td>(ROOT (S (S (NP (PRP I)) (VP (VBP 'm) (ADJP (J...</td>\n",
       "      <td>verbatim</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    promptID   pairID                                            premise  \\\n",
       "0      75290   75290c  Your contribution helped make it possible for ...   \n",
       "1     133794  133794c  The answer has nothing to do with their cause,...   \n",
       "2       3628    3628c   We serve a classic Tuscan meal that includes ...   \n",
       "3      89411   89411c  A few months ago, Carl Newton and I wrote a le...   \n",
       "4     136158  136158e  I was on this earth you know, I've lived on th...   \n",
       "..       ...      ...                                                ...   \n",
       "92     75857   75857c  Recently we read that how children turn out is...   \n",
       "93    101420  101420n  And I said to some of my cousins that I had on...   \n",
       "94    131793  131793e  He views either of them as the instrument, the...   \n",
       "95    121697  121697e                          Something to that effect.   \n",
       "96      3901    3901c  I do not know how he would rate it, but I must...   \n",
       "\n",
       "                                 premise_binary_parse  \\\n",
       "0   ( ( Your contribution ) ( ( helped ( make ( it...   \n",
       "1   ( ( ( ( ( ( The answer ) ( ( ( ( has nothing )...   \n",
       "2   ( We ( ( serve ( ( a ( classic ( Tuscan meal )...   \n",
       "3   ( ( ( A ( few months ) ) ago ) ( , ( ( ( ( Car...   \n",
       "4   ( I ( ( was ( on ( ( this earth ) ( you ( know...   \n",
       "..                                                ...   \n",
       "92  ( ( ( ( Recently ( we ( read ( that ( ( how ( ...   \n",
       "93  ( And ( I ( ( ( said ( to ( some ( of ( my cou...   \n",
       "94  ( He ( ( ( views ( either ( of them ) ) ) ( as...   \n",
       "95         ( ( Something ( to ( that effect ) ) ) . )   \n",
       "96  ( ( ( ( ( I ( ( do not ) ( know ( how ( he ( w...   \n",
       "\n",
       "                                        premise_parse  \\\n",
       "0   (ROOT (S (NP (PRP$ Your) (NN contribution)) (V...   \n",
       "1   (ROOT (S (S (NP (DT The) (NN answer)) (VP (VBZ...   \n",
       "2   (ROOT (S (NP (PRP We)) (VP (VBP serve) (NP (NP...   \n",
       "3   (ROOT (S (ADVP (NP (DT A) (JJ few) (NNS months...   \n",
       "4   (ROOT (S (NP (PRP I)) (VP (VBD was) (PP (IN on...   \n",
       "..                                                ...   \n",
       "92  (ROOT (S (S (ADVP (RB Recently)) (NP (PRP we))...   \n",
       "93  (ROOT (S (CC And) (NP (PRP I)) (VP (VBD said) ...   \n",
       "94  (ROOT (S (NP (PRP He)) (VP (VBZ views) (NP (NP...   \n",
       "95  (ROOT (NP (NP (NN Something)) (PP (TO to) (NP ...   \n",
       "96  (ROOT (S (S (NP (PRP I)) (VP (VBP do) (RB not)...   \n",
       "\n",
       "                                           hypothesis  \\\n",
       "0   Your contributions were of no help with our st...   \n",
       "1   Dictionaries are indeed exercises in bi-unique...   \n",
       "2              We serve a meal of Florentine terrine.   \n",
       "3   Carl Newton and I have never had any other pre...   \n",
       "4   I don't yet know the reason why I have lived o...   \n",
       "..                                                ...   \n",
       "92  How children turn out is primarily determined ...   \n",
       "93  The last time I saw my nieces were at grandfat...   \n",
       "94     He cannot pass the klngship onto someone else.   \n",
       "95               That's approximately what it means.    \n",
       "96  I'm sure he'd consider Chambers to be better, ...   \n",
       "\n",
       "                              hypothesis_binary_parse  \\\n",
       "0   ( ( Your contributions ) ( ( were ( of ( ( no ...   \n",
       "1   ( Dictionaries ( ( ( are indeed ) ( exercises ...   \n",
       "2   ( We ( ( serve ( ( a meal ) ( of ( Florentine ...   \n",
       "3   ( ( ( ( Carl Newton ) and ) I ) ( ( ( have nev...   \n",
       "4   ( I ( ( ( ( do n't ) yet ) ( ( know ( the reas...   \n",
       "..                                                ...   \n",
       "92  ( ( How ( children ( turn out ) ) ) ( ( ( is p...   \n",
       "93  ( ( ( The ( last time ) ) ( I ( saw ( my niece...   \n",
       "94  ( He ( ( ( can not ) ( ( pass ( the klngship )...   \n",
       "95  ( That ( ( 's ( approximately ( what ( it mean...   \n",
       "96  ( ( ( ( ( I ( 'm ( sure ( he ( 'd ( consider (...   \n",
       "\n",
       "                                     hypothesis_parse       genre  label  \n",
       "0   (ROOT (S (NP (PRP$ Your) (NNS contributions)) ...     letters      2  \n",
       "1   (ROOT (S (NP (NNS Dictionaries)) (VP (VBP are)...    verbatim      2  \n",
       "2   (ROOT (S (NP (PRP We)) (VP (VBP serve) (NP (NP...    verbatim      0  \n",
       "3   (ROOT (S (NP (NP (NNP Carl) (NNP Newton)) (CC ...     letters      2  \n",
       "4   (ROOT (S (NP (PRP I)) (VP (VBP do) (RB n't) (A...  facetoface      0  \n",
       "..                                                ...         ...    ...  \n",
       "92  (ROOT (S (SBAR (WHADVP (WRB How)) (S (NP (NNS ...         oup      2  \n",
       "93  (ROOT (S (NP (NP (DT The) (JJ last) (NN time))...  facetoface      1  \n",
       "94  (ROOT (S (NP (PRP He)) (VP (MD can) (RB not) (...    verbatim      0  \n",
       "95  (ROOT (S (NP (DT That)) (VP (VBZ 's) (ADJP (RB...  facetoface      0  \n",
       "96  (ROOT (S (S (NP (PRP I)) (VP (VBP 'm) (ADJP (J...    verbatim      1  \n",
       "\n",
       "[97 rows x 10 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T17:39:47.575638200Z",
     "start_time": "2024-02-03T17:39:47.045637400Z"
    },
    "id": "Gk96lNh94_e_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pickle\n",
    "import os\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class MNLIDataBert(Dataset):\n",
    "\n",
    "  def __init__(self, train_df, val_df):\n",
    "    #self.label_dict = {'entailment': 0, 'contradiction': 1, 'neutral': 2}\n",
    "\n",
    "    self.train_df = train_df\n",
    "    self.val_df = val_df\n",
    "\n",
    "    self.base_path = '/content/'\n",
    "    self.tokenizer = BertTokenizer.from_pretrained('Speedtest/bert_weights_speedtest', do_lower_case=True)\n",
    "    self.train_data = None\n",
    "    self.val_data = None\n",
    "    self.init_data()\n",
    "\n",
    "  def init_data(self):\n",
    "    # Saving takes too much RAM\n",
    "    #\n",
    "    # if os.path.exists(os.path.join(self.base_path, 'train_data.pkl')):\n",
    "    #   print(\"Found training data\")\n",
    "    #   with open(os.path.join(self.base_path, 'train_data.pkl'), 'rb') as f:\n",
    "    #     self.train_data = pickle.load(f)\n",
    "    # else:\n",
    "    #   self.train_data = self.load_data(self.train_df)\n",
    "    #   with open(os.path.join(self.base_path, 'train_data.pkl'), 'wb') as f:\n",
    "    #     pickle.dump(self.train_data, f)\n",
    "    # if os.path.exists(os.path.join(self.base_path, 'val_data.pkl')):\n",
    "    #   print(\"Found val data\")\n",
    "    #   with open(os.path.join(self.base_path, 'val_data.pkl'), 'rb') as f:\n",
    "    #     self.val_data = pickle.load(f)\n",
    "    # else:\n",
    "    #   self.val_data = self.load_data(self.val_df)\n",
    "    #   with open(os.path.join(self.base_path, 'val_data.pkl'), 'wb') as f:\n",
    "    #     pickle.dump(self.val_data, f)\n",
    "    self.train_data = self.load_data(self.train_df)\n",
    "    self.val_data = self.load_data(self.val_df)\n",
    "\n",
    "  def load_data(self, df):\n",
    "    MAX_LEN = 512\n",
    "    token_ids = []\n",
    "    mask_ids = []\n",
    "    seg_ids = []\n",
    "    y = []\n",
    "\n",
    "    premise_list = df['premise'].to_list()\n",
    "    hypothesis_list = df['hypothesis'].to_list()\n",
    "    label_list = df['label'].to_list()\n",
    "    #print(label_list)\n",
    "\n",
    "    for (_premise, _hypothesis, _label) in zip(premise_list, hypothesis_list, label_list):\n",
    "      premise_id = self.tokenizer.encode(_premise, add_special_tokens = False)\n",
    "      hypothesis_id = self.tokenizer.encode(_hypothesis, add_special_tokens = False)\n",
    "      pair_token_ids = [self.tokenizer.cls_token_id] + premise_id + [self.tokenizer.sep_token_id] + hypothesis_id + [self.tokenizer.sep_token_id]\n",
    "      premise_len = len(premise_id)\n",
    "      hypothesis_len = len(hypothesis_id)\n",
    "\n",
    "      segment_ids = torch.tensor([0] * (premise_len + 2) + [1] * (hypothesis_len + 1))  # sentence 0 and sentence 1\n",
    "      attention_mask_ids = torch.tensor([1] * (premise_len + hypothesis_len + 3))  # mask padded values\n",
    "\n",
    "      token_ids.append(torch.tensor(pair_token_ids))\n",
    "      seg_ids.append(segment_ids)\n",
    "      mask_ids.append(attention_mask_ids)\n",
    "      # print(_label)\n",
    "      # print(self.label_dict.keys())\n",
    "      y.append(_label)\n",
    "    \n",
    "    token_ids = pad_sequence(token_ids, batch_first=True)\n",
    "    mask_ids = pad_sequence(mask_ids, batch_first=True)\n",
    "    seg_ids = pad_sequence(seg_ids, batch_first=True)\n",
    "    y = torch.tensor(y)\n",
    "    dataset = TensorDataset(token_ids, mask_ids, seg_ids, y)\n",
    "    print(len(dataset))\n",
    "    return dataset\n",
    "\n",
    "  def get_data_loaders(self, batch_size=32, shuffle=True):\n",
    "    train_loader = DataLoader(\n",
    "      self.train_data,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "      self.val_data,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T17:39:47.840636400Z",
     "start_time": "2024-02-03T17:39:47.580643800Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "md52P1z14_e_",
    "outputId": "3421aec1-128f-4589-874d-a82372138365"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "97\n"
     ]
    }
   ],
   "source": [
    "mnli_dataset = MNLIDataBert(train_df, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T17:39:47.854636Z",
     "start_time": "2024-02-03T17:39:47.845637400Z"
    },
    "id": "-LSyABLG4_fA"
   },
   "outputs": [],
   "source": [
    "train_loader, val_loader = mnli_dataset.get_data_loaders(batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T17:39:49.098715600Z",
     "start_time": "2024-02-03T17:39:47.847635900Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qOdc4Cs2DEjt",
    "outputId": "98759f4f-ead1-4b1d-c20c-0ce4996de3ed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert_weights_v3 were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert_weights_v3 and are newly initialized: ['bert.pooler.dense.weight', 'classifier.bias', 'bert.pooler.dense.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert_weights_v3\", num_labels=3)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T17:39:49.134737700Z",
     "start_time": "2024-02-03T17:39:49.091727700Z"
    },
    "id": "jxzpENXlEh-u"
   },
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T17:39:49.135716200Z",
     "start_time": "2024-02-03T17:39:49.091727700Z"
    },
    "id": "is1TqwTREid9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haehnel/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# This variable contains all of the hyperparemeter information our training loop needs\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, correct_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T17:39:49.136746200Z",
     "start_time": "2024-02-03T17:39:49.092715600Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "79aI1dun4_fB",
    "outputId": "c2f15bac-22d1-403b-cc1b-0a758cae0433"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 109,484,547 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T17:39:49.136746200Z",
     "start_time": "2024-02-03T17:39:49.093716700Z"
    },
    "id": "qhU855Cw4_fB"
   },
   "outputs": [],
   "source": [
    "def multi_acc(y_pred, y_test):\n",
    "  acc = (torch.log_softmax(y_pred, dim=1).argmax(dim=1) == y_test).sum().float() / float(y_test.size(0))\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T17:39:49.260716500Z",
     "start_time": "2024-02-03T17:39:49.102716Z"
    },
    "id": "OfhYO7Db4_fB"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer):  \n",
    "  total_step = len(train_loader)\n",
    "\n",
    "  for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_train_acc  = 0\n",
    "    for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in enumerate(train_loader):\n",
    "      optimizer.zero_grad()\n",
    "      pair_token_ids = pair_token_ids.to(device)\n",
    "      mask_ids = mask_ids.to(device)\n",
    "      seg_ids = seg_ids.to(device)\n",
    "      labels = y.to(device)\n",
    "      # prediction = model(pair_token_ids, mask_ids, seg_ids)\n",
    "      loss, prediction = model(pair_token_ids, \n",
    "                             token_type_ids=seg_ids, \n",
    "                             attention_mask=mask_ids, \n",
    "                             labels=labels).values()\n",
    "\n",
    "      # loss = criterion(prediction, labels)\n",
    "      acc = multi_acc(prediction, labels)\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      \n",
    "      total_train_loss += loss.item()\n",
    "      total_train_acc  += acc.item()\n",
    "\n",
    "    train_acc  = total_train_acc/len(train_loader)\n",
    "    train_loss = total_train_loss/len(train_loader)\n",
    "    model.eval()\n",
    "    total_val_acc  = 0\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "      for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in enumerate(val_loader):\n",
    "        optimizer.zero_grad()\n",
    "        pair_token_ids = pair_token_ids.to(device)\n",
    "        mask_ids = mask_ids.to(device)\n",
    "        seg_ids = seg_ids.to(device)\n",
    "        labels = y.to(device)\n",
    "\n",
    "        # prediction = model(pair_token_ids, mask_ids, seg_ids)\n",
    "        loss, prediction = model(pair_token_ids, \n",
    "                             token_type_ids=seg_ids, \n",
    "                             attention_mask=mask_ids, \n",
    "                             labels=labels).values()\n",
    "        \n",
    "        # loss = criterion(prediction, labels)\n",
    "        acc = multi_acc(prediction, labels)\n",
    "\n",
    "        total_val_loss += loss.item()\n",
    "        total_val_acc  += acc.item()\n",
    "\n",
    "    val_acc  = total_val_acc/len(val_loader)\n",
    "    val_loss = total_val_loss/len(val_loader)\n",
    "    end = time.time()\n",
    "    hours, rem = divmod(end-start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "    print(f'Epoch {epoch+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
    "    print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T17:47:45.065299300Z",
     "start_time": "2024-02-03T17:39:49.150716400Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GPIxT4RB4_fC",
    "outputId": "4909515f-1869-45da-cf0e-8491493067b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss: 1.1281 train_acc: 0.4196 | val_loss: 1.5363 val_acc: 0.5000\n",
      "00:01:38.17\n",
      "Epoch 2: train_loss: 1.4335 train_acc: 0.2857 | val_loss: 1.1394 val_acc: 0.2321\n",
      "00:01:33.02\n",
      "Epoch 3: train_loss: 1.2104 train_acc: 0.3304 | val_loss: 1.1065 val_acc: 0.4018\n",
      "00:01:35.12\n",
      "Epoch 4: train_loss: 1.1414 train_acc: 0.3571 | val_loss: 1.1872 val_acc: 0.2321\n",
      "00:01:34.85\n",
      "Epoch 5: train_loss: 1.0816 train_acc: 0.4286 | val_loss: 1.1197 val_acc: 0.2679\n",
      "00:01:34.75\n",
      "Fine-tuning with 1% of the data took 7 minutes and 55 seconds to execute.\n",
      " Based on that,  finetuning with 100% will take 793.1811813513438 minutes and 10.870881080627441 seconds to complete.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def my_method():\n",
    "    # Your method's code goes here\n",
    "    time.sleep(125)  # Simulating some work\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "train(model, train_loader, val_loader, optimizer)\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Convert to minutes and seconds\n",
    "elapsed_minutes = int(elapsed_time / 60)\n",
    "elapsed_seconds = int(elapsed_time % 60)\n",
    "\n",
    "print(f\"Fine-tuning with 1% of the data took {elapsed_minutes} minutes and {elapsed_seconds} seconds to execute.\\n Based on that,  finetuning with 100% will take {elapsed_time * 100 /60} minutes and {elapsed_time * 100 % 60} seconds to complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T17:47:45.066269600Z",
     "start_time": "2024-02-03T17:47:45.063264100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Sentence Entailment BERT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
