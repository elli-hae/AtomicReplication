{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "Modified from [Sentence-Entailment repository](https://colab.research.google.com/github/dh1105/Sentence-Entailment/blob/main/Sentence_Entailment_BERT.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T20:07:07.403579900Z",
     "start_time": "2024-02-03T20:07:07.226523500Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uqcn3_lQ5Hhr",
    "outputId": "3ee374ba-39ca-472e-d392-bdb4f6e4638e"
   },
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "#!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T20:07:08.603260500Z",
     "start_time": "2024-02-03T20:07:07.397522400Z"
    },
    "id": "JQ4M8Znd4_ep"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "# import torch_xla\n",
    "# import torch_xla.core.xla_model as xm\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, SequentialSampler, RandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T20:07:08.644300100Z",
     "start_time": "2024-02-03T20:07:08.643279100Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WkpfPDS1Z8a8",
    "outputId": "3219bc62-f97b-499f-a28a-0379e1560f71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T20:07:10.490677500Z",
     "start_time": "2024-02-03T20:07:08.644300100Z"
    },
    "id": "9vYZA1zq4_e7"
   },
   "outputs": [],
   "source": [
    "# 70/30 split\n",
    "train_df = pd.read_parquet('../data/train-00000-of-00001.parquet')[:3500]\n",
    "val_df   = pd.read_parquet('../data/validation_mismatched-00000-of-00001.parquet')[:1050]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T20:07:10.758662300Z",
     "start_time": "2024-02-03T20:07:10.487652500Z"
    },
    "id": "3NgPYXBg4_e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre', 'label']\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df.dropna()\n",
    "val_df = val_df.dropna()\n",
    "print(train_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T20:07:10.769684200Z",
     "start_time": "2024-02-03T20:07:10.759662900Z"
    },
    "id": "UMTQb1iMF54W"
   },
   "outputs": [],
   "source": [
    "train_df['premise'] = train_df['premise'].astype(str)\n",
    "train_df['hypothesis'] = train_df['hypothesis'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T20:07:10.778687400Z",
     "start_time": "2024-02-03T20:07:10.766669400Z"
    },
    "id": "GMzPNmtpF8yg"
   },
   "outputs": [],
   "source": [
    "val_df['premise'] = val_df['premise'].astype(str)\n",
    "val_df['hypothesis'] = val_df['hypothesis'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T20:07:10.802662100Z",
     "start_time": "2024-02-03T20:07:10.774662200Z"
    },
    "id": "diS3wCm14_e9"
   },
   "outputs": [],
   "source": [
    "train_df = train_df[(train_df['premise'].str.split().str.len() > 0) & (train_df['hypothesis'].str.split().str.len() > 0)]\n",
    "val_df = val_df[(val_df['premise'].str.split().str.len() > 0) & (val_df['hypothesis'].str.split().str.len() > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T20:07:10.856688500Z",
     "start_time": "2024-02-03T20:07:10.808663700Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "id": "WinUrXE04_e9",
    "outputId": "974ce2c2-04b4-40e2-f33d-ee172eb6c49e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>promptID</th>\n",
       "      <th>pairID</th>\n",
       "      <th>premise</th>\n",
       "      <th>premise_binary_parse</th>\n",
       "      <th>premise_parse</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>hypothesis_binary_parse</th>\n",
       "      <th>hypothesis_parse</th>\n",
       "      <th>genre</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31193</td>\n",
       "      <td>31193n</td>\n",
       "      <td>Conceptually cream skimming has two basic dime...</td>\n",
       "      <td>( ( Conceptually ( cream skimming ) ) ( ( has ...</td>\n",
       "      <td>(ROOT (S (NP (JJ Conceptually) (NN cream) (NN ...</td>\n",
       "      <td>Product and geography are what make cream skim...</td>\n",
       "      <td>( ( ( Product and ) geography ) ( ( are ( what...</td>\n",
       "      <td>(ROOT (S (NP (NN Product) (CC and) (NN geograp...</td>\n",
       "      <td>government</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101457</td>\n",
       "      <td>101457e</td>\n",
       "      <td>you know during the season and i guess at at y...</td>\n",
       "      <td>( you ( ( know ( during ( ( ( the season ) and...</td>\n",
       "      <td>(ROOT (S (NP (PRP you)) (VP (VBP know) (PP (IN...</td>\n",
       "      <td>You lose the things to the following level if ...</td>\n",
       "      <td>( You ( ( ( ( lose ( the things ) ) ( to ( the...</td>\n",
       "      <td>(ROOT (S (NP (PRP You)) (VP (VBP lose) (NP (DT...</td>\n",
       "      <td>telephone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>134793</td>\n",
       "      <td>134793e</td>\n",
       "      <td>One of our number will carry out your instruct...</td>\n",
       "      <td>( ( One ( of ( our number ) ) ) ( ( will ( ( (...</td>\n",
       "      <td>(ROOT (S (NP (NP (CD One)) (PP (IN of) (NP (PR...</td>\n",
       "      <td>A member of my team will execute your orders w...</td>\n",
       "      <td>( ( ( A member ) ( of ( my team ) ) ) ( ( will...</td>\n",
       "      <td>(ROOT (S (NP (NP (DT A) (NN member)) (PP (IN o...</td>\n",
       "      <td>fiction</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37397</td>\n",
       "      <td>37397e</td>\n",
       "      <td>How do you know? All this is their information...</td>\n",
       "      <td>( ( How ( ( ( do you ) know ) ? ) ) ( ( All th...</td>\n",
       "      <td>(ROOT (S (SBARQ (WHADVP (WRB How)) (SQ (VBP do...</td>\n",
       "      <td>This information belongs to them.</td>\n",
       "      <td>( ( This information ) ( ( belongs ( to them )...</td>\n",
       "      <td>(ROOT (S (NP (DT This) (NN information)) (VP (...</td>\n",
       "      <td>fiction</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50563</td>\n",
       "      <td>50563n</td>\n",
       "      <td>yeah i tell you what though if you go price so...</td>\n",
       "      <td>( yeah ( i ( ( tell you ) ( what ( ( though ( ...</td>\n",
       "      <td>(ROOT (S (VP (VB yeah) (S (NP (FW i)) (VP (VB ...</td>\n",
       "      <td>The tennis shoes have a range of prices.</td>\n",
       "      <td>( ( The ( tennis shoes ) ) ( ( have ( ( a rang...</td>\n",
       "      <td>(ROOT (S (NP (DT The) (NN tennis) (NNS shoes))...</td>\n",
       "      <td>telephone</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3495</th>\n",
       "      <td>126476</td>\n",
       "      <td>126476n</td>\n",
       "      <td>They actually seem to play because they love t...</td>\n",
       "      <td>( They ( actually ( ( seem ( to ( play ( becau...</td>\n",
       "      <td>(ROOT (S (NP (PRP They)) (ADVP (RB actually)) ...</td>\n",
       "      <td>They seem to play because of their love of the...</td>\n",
       "      <td>( They ( ( seem ( to ( play ( ( ( because ( of...</td>\n",
       "      <td>(ROOT (S (NP (PRP They)) (VP (VBP seem) (S (VP...</td>\n",
       "      <td>slate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3496</th>\n",
       "      <td>49514</td>\n",
       "      <td>49514e</td>\n",
       "      <td>because i i just don't have that discipline wi...</td>\n",
       "      <td>( because ( ( ( i i ) just ) ( ( do n't ) ( ( ...</td>\n",
       "      <td>(ROOT (SBAR (IN because) (S (NP (NP (FW i) (FW...</td>\n",
       "      <td>Because I don't know how to get children to ob...</td>\n",
       "      <td>( ( Because ( I ( do n't ) ) ) ( ( know ( how ...</td>\n",
       "      <td>(ROOT (S (SBAR (IN Because) (S (NP (PRP I)) (V...</td>\n",
       "      <td>telephone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3497</th>\n",
       "      <td>45316</td>\n",
       "      <td>45316e</td>\n",
       "      <td>well how did you get a recipe for pudding on y...</td>\n",
       "      <td>( ( well how ) ( ( did you ) ( ( get ( ( a rec...</td>\n",
       "      <td>(ROOT (SBARQ (WHADVP (RB well) (WRB how)) (SQ ...</td>\n",
       "      <td>How did you get a recipe for pudding?</td>\n",
       "      <td>( How ( ( ( did you ) ( get ( ( a recipe ) ( f...</td>\n",
       "      <td>(ROOT (SBARQ (WHADVP (WRB How)) (SQ (VBD did) ...</td>\n",
       "      <td>telephone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3498</th>\n",
       "      <td>60233</td>\n",
       "      <td>60233n</td>\n",
       "      <td>and there's some freebies out there are you sh...</td>\n",
       "      <td>( and ( there ( 's ( some ( ( freebies ( out (...</td>\n",
       "      <td>(ROOT (PRN (CC and) (S (NP (EX there)) (VP (VB...</td>\n",
       "      <td>The savings account takes exactly 12.5 percent...</td>\n",
       "      <td>( ( The ( savings account ) ) ( ( takes ( ( ex...</td>\n",
       "      <td>(ROOT (S (NP (DT The) (NNS savings) (NN accoun...</td>\n",
       "      <td>telephone</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3499</th>\n",
       "      <td>139095</td>\n",
       "      <td>139095e</td>\n",
       "      <td>what you see is what you get editor for almost...</td>\n",
       "      <td>( ( ( what ( you ( see ( ( is ( what ( you ( (...</td>\n",
       "      <td>(ROOT (UCP (SBAR (WHNP (WP what)) (S (NP (PRP ...</td>\n",
       "      <td>I use some word processors at home.</td>\n",
       "      <td>( I ( ( ( use ( some ( word processors ) ) ) (...</td>\n",
       "      <td>(ROOT (S (NP (PRP I)) (VP (VBP use) (NP (DT so...</td>\n",
       "      <td>telephone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3500 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      promptID   pairID                                            premise  \\\n",
       "0        31193   31193n  Conceptually cream skimming has two basic dime...   \n",
       "1       101457  101457e  you know during the season and i guess at at y...   \n",
       "2       134793  134793e  One of our number will carry out your instruct...   \n",
       "3        37397   37397e  How do you know? All this is their information...   \n",
       "4        50563   50563n  yeah i tell you what though if you go price so...   \n",
       "...        ...      ...                                                ...   \n",
       "3495    126476  126476n  They actually seem to play because they love t...   \n",
       "3496     49514   49514e  because i i just don't have that discipline wi...   \n",
       "3497     45316   45316e  well how did you get a recipe for pudding on y...   \n",
       "3498     60233   60233n  and there's some freebies out there are you sh...   \n",
       "3499    139095  139095e  what you see is what you get editor for almost...   \n",
       "\n",
       "                                   premise_binary_parse  \\\n",
       "0     ( ( Conceptually ( cream skimming ) ) ( ( has ...   \n",
       "1     ( you ( ( know ( during ( ( ( the season ) and...   \n",
       "2     ( ( One ( of ( our number ) ) ) ( ( will ( ( (...   \n",
       "3     ( ( How ( ( ( do you ) know ) ? ) ) ( ( All th...   \n",
       "4     ( yeah ( i ( ( tell you ) ( what ( ( though ( ...   \n",
       "...                                                 ...   \n",
       "3495  ( They ( actually ( ( seem ( to ( play ( becau...   \n",
       "3496  ( because ( ( ( i i ) just ) ( ( do n't ) ( ( ...   \n",
       "3497  ( ( well how ) ( ( did you ) ( ( get ( ( a rec...   \n",
       "3498  ( and ( there ( 's ( some ( ( freebies ( out (...   \n",
       "3499  ( ( ( what ( you ( see ( ( is ( what ( you ( (...   \n",
       "\n",
       "                                          premise_parse  \\\n",
       "0     (ROOT (S (NP (JJ Conceptually) (NN cream) (NN ...   \n",
       "1     (ROOT (S (NP (PRP you)) (VP (VBP know) (PP (IN...   \n",
       "2     (ROOT (S (NP (NP (CD One)) (PP (IN of) (NP (PR...   \n",
       "3     (ROOT (S (SBARQ (WHADVP (WRB How)) (SQ (VBP do...   \n",
       "4     (ROOT (S (VP (VB yeah) (S (NP (FW i)) (VP (VB ...   \n",
       "...                                                 ...   \n",
       "3495  (ROOT (S (NP (PRP They)) (ADVP (RB actually)) ...   \n",
       "3496  (ROOT (SBAR (IN because) (S (NP (NP (FW i) (FW...   \n",
       "3497  (ROOT (SBARQ (WHADVP (RB well) (WRB how)) (SQ ...   \n",
       "3498  (ROOT (PRN (CC and) (S (NP (EX there)) (VP (VB...   \n",
       "3499  (ROOT (UCP (SBAR (WHNP (WP what)) (S (NP (PRP ...   \n",
       "\n",
       "                                             hypothesis  \\\n",
       "0     Product and geography are what make cream skim...   \n",
       "1     You lose the things to the following level if ...   \n",
       "2     A member of my team will execute your orders w...   \n",
       "3                     This information belongs to them.   \n",
       "4              The tennis shoes have a range of prices.   \n",
       "...                                                 ...   \n",
       "3495  They seem to play because of their love of the...   \n",
       "3496  Because I don't know how to get children to ob...   \n",
       "3497              How did you get a recipe for pudding?   \n",
       "3498  The savings account takes exactly 12.5 percent...   \n",
       "3499                I use some word processors at home.   \n",
       "\n",
       "                                hypothesis_binary_parse  \\\n",
       "0     ( ( ( Product and ) geography ) ( ( are ( what...   \n",
       "1     ( You ( ( ( ( lose ( the things ) ) ( to ( the...   \n",
       "2     ( ( ( A member ) ( of ( my team ) ) ) ( ( will...   \n",
       "3     ( ( This information ) ( ( belongs ( to them )...   \n",
       "4     ( ( The ( tennis shoes ) ) ( ( have ( ( a rang...   \n",
       "...                                                 ...   \n",
       "3495  ( They ( ( seem ( to ( play ( ( ( because ( of...   \n",
       "3496  ( ( Because ( I ( do n't ) ) ) ( ( know ( how ...   \n",
       "3497  ( How ( ( ( did you ) ( get ( ( a recipe ) ( f...   \n",
       "3498  ( ( The ( savings account ) ) ( ( takes ( ( ex...   \n",
       "3499  ( I ( ( ( use ( some ( word processors ) ) ) (...   \n",
       "\n",
       "                                       hypothesis_parse       genre  label  \n",
       "0     (ROOT (S (NP (NN Product) (CC and) (NN geograp...  government      1  \n",
       "1     (ROOT (S (NP (PRP You)) (VP (VBP lose) (NP (DT...   telephone      0  \n",
       "2     (ROOT (S (NP (NP (DT A) (NN member)) (PP (IN o...     fiction      0  \n",
       "3     (ROOT (S (NP (DT This) (NN information)) (VP (...     fiction      0  \n",
       "4     (ROOT (S (NP (DT The) (NN tennis) (NNS shoes))...   telephone      1  \n",
       "...                                                 ...         ...    ...  \n",
       "3495  (ROOT (S (NP (PRP They)) (VP (VBP seem) (S (VP...       slate      1  \n",
       "3496  (ROOT (S (SBAR (IN Because) (S (NP (PRP I)) (V...   telephone      0  \n",
       "3497  (ROOT (SBARQ (WHADVP (WRB How)) (SQ (VBD did) ...   telephone      0  \n",
       "3498  (ROOT (S (NP (DT The) (NNS savings) (NN accoun...   telephone      1  \n",
       "3499  (ROOT (S (NP (PRP I)) (VP (VBP use) (NP (DT so...   telephone      0  \n",
       "\n",
       "[3500 rows x 10 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T20:07:10.921661800Z",
     "start_time": "2024-02-03T20:07:10.855660900Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "id": "6e4dxTks4_e-",
    "outputId": "d6e58040-d442-48e3-9f44-563dca5b9031"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>promptID</th>\n",
       "      <th>pairID</th>\n",
       "      <th>premise</th>\n",
       "      <th>premise_binary_parse</th>\n",
       "      <th>premise_parse</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>hypothesis_binary_parse</th>\n",
       "      <th>hypothesis_parse</th>\n",
       "      <th>genre</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75290</td>\n",
       "      <td>75290c</td>\n",
       "      <td>Your contribution helped make it possible for ...</td>\n",
       "      <td>( ( Your contribution ) ( ( helped ( make ( it...</td>\n",
       "      <td>(ROOT (S (NP (PRP$ Your) (NN contribution)) (V...</td>\n",
       "      <td>Your contributions were of no help with our st...</td>\n",
       "      <td>( ( Your contributions ) ( ( were ( of ( ( no ...</td>\n",
       "      <td>(ROOT (S (NP (PRP$ Your) (NNS contributions)) ...</td>\n",
       "      <td>letters</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>133794</td>\n",
       "      <td>133794c</td>\n",
       "      <td>The answer has nothing to do with their cause,...</td>\n",
       "      <td>( ( ( ( ( ( The answer ) ( ( ( ( has nothing )...</td>\n",
       "      <td>(ROOT (S (S (NP (DT The) (NN answer)) (VP (VBZ...</td>\n",
       "      <td>Dictionaries are indeed exercises in bi-unique...</td>\n",
       "      <td>( Dictionaries ( ( ( are indeed ) ( exercises ...</td>\n",
       "      <td>(ROOT (S (NP (NNS Dictionaries)) (VP (VBP are)...</td>\n",
       "      <td>verbatim</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3628</td>\n",
       "      <td>3628c</td>\n",
       "      <td>We serve a classic Tuscan meal that includes ...</td>\n",
       "      <td>( We ( ( serve ( ( a ( classic ( Tuscan meal )...</td>\n",
       "      <td>(ROOT (S (NP (PRP We)) (VP (VBP serve) (NP (NP...</td>\n",
       "      <td>We serve a meal of Florentine terrine.</td>\n",
       "      <td>( We ( ( serve ( ( a meal ) ( of ( Florentine ...</td>\n",
       "      <td>(ROOT (S (NP (PRP We)) (VP (VBP serve) (NP (NP...</td>\n",
       "      <td>verbatim</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89411</td>\n",
       "      <td>89411c</td>\n",
       "      <td>A few months ago, Carl Newton and I wrote a le...</td>\n",
       "      <td>( ( ( A ( few months ) ) ago ) ( , ( ( ( ( Car...</td>\n",
       "      <td>(ROOT (S (ADVP (NP (DT A) (JJ few) (NNS months...</td>\n",
       "      <td>Carl Newton and I have never had any other pre...</td>\n",
       "      <td>( ( ( ( Carl Newton ) and ) I ) ( ( ( have nev...</td>\n",
       "      <td>(ROOT (S (NP (NP (NNP Carl) (NNP Newton)) (CC ...</td>\n",
       "      <td>letters</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>136158</td>\n",
       "      <td>136158e</td>\n",
       "      <td>I was on this earth you know, I've lived on th...</td>\n",
       "      <td>( I ( ( was ( on ( ( this earth ) ( you ( know...</td>\n",
       "      <td>(ROOT (S (NP (PRP I)) (VP (VBD was) (PP (IN on...</td>\n",
       "      <td>I don't yet know the reason why I have lived o...</td>\n",
       "      <td>( I ( ( ( ( do n't ) yet ) ( ( know ( the reas...</td>\n",
       "      <td>(ROOT (S (NP (PRP I)) (VP (VBP do) (RB n't) (A...</td>\n",
       "      <td>facetoface</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>66729</td>\n",
       "      <td>66729e</td>\n",
       "      <td>That's a little bit of pressure, though.</td>\n",
       "      <td>( That ( ( ( ( 's ( ( a ( little bit ) ) ( of ...</td>\n",
       "      <td>(ROOT (S (NP (DT That)) (VP (VBZ 's) (NP (NP (...</td>\n",
       "      <td>That is a small amount of pressure.</td>\n",
       "      <td>( That ( ( is ( ( a ( small amount ) ) ( of pr...</td>\n",
       "      <td>(ROOT (S (NP (DT That)) (VP (VBZ is) (NP (NP (...</td>\n",
       "      <td>facetoface</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>53477</td>\n",
       "      <td>53477n</td>\n",
       "      <td>But in most situations in which a firm provide...</td>\n",
       "      <td>( But ( ( in ( ( most situations ) ( ( in whic...</td>\n",
       "      <td>(ROOT (S (CC But) (PP (IN in) (NP (NP (JJS mos...</td>\n",
       "      <td>The high demand variation products are typical...</td>\n",
       "      <td>( ( The ( high ( demand ( variation products )...</td>\n",
       "      <td>(ROOT (S (NP (DT The) (JJ high) (NN demand) (N...</td>\n",
       "      <td>oup</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>92836</td>\n",
       "      <td>92836n</td>\n",
       "      <td>Go ahead Adam.</td>\n",
       "      <td>( ( ( Go ahead ) Adam ) . )</td>\n",
       "      <td>(ROOT (S (VP (VB Go) (PRT (RP ahead)) (NP (NNP...</td>\n",
       "      <td>You can win the tournament Adam.</td>\n",
       "      <td>( You ( ( can ( win ( the ( tournament Adam ) ...</td>\n",
       "      <td>(ROOT (S (NP (PRP You)) (VP (MD can) (VP (VB w...</td>\n",
       "      <td>facetoface</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>117942</td>\n",
       "      <td>117942c</td>\n",
       "      <td>They could not recall that any of the passenge...</td>\n",
       "      <td>( They ( ( ( could not ) ( recall ( that ( ( a...</td>\n",
       "      <td>(ROOT (S (NP (PRP They)) (VP (MD could) (RB no...</td>\n",
       "      <td>They did not screen any passengers, and had no...</td>\n",
       "      <td>( They ( ( ( ( ( ( did not ) ( screen ( any pa...</td>\n",
       "      <td>(ROOT (S (NP (PRP They)) (VP (VP (VBD did) (RB...</td>\n",
       "      <td>nineeleven</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>120094</td>\n",
       "      <td>120094n</td>\n",
       "      <td>Others who worked with ACM's high school progr...</td>\n",
       "      <td>( ( Others ( who ( ( ( ( ( worked ( with ( ( A...</td>\n",
       "      <td>(ROOT (S (NP (NP (NNS Others)) (SBAR (WHNP (WP...</td>\n",
       "      <td>These traditions are important to school culture.</td>\n",
       "      <td>( ( These traditions ) ( ( are ( important ( t...</td>\n",
       "      <td>(ROOT (S (NP (DT These) (NNS traditions)) (VP ...</td>\n",
       "      <td>letters</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1050 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      promptID   pairID                                            premise  \\\n",
       "0        75290   75290c  Your contribution helped make it possible for ...   \n",
       "1       133794  133794c  The answer has nothing to do with their cause,...   \n",
       "2         3628    3628c   We serve a classic Tuscan meal that includes ...   \n",
       "3        89411   89411c  A few months ago, Carl Newton and I wrote a le...   \n",
       "4       136158  136158e  I was on this earth you know, I've lived on th...   \n",
       "...        ...      ...                                                ...   \n",
       "1045     66729   66729e           That's a little bit of pressure, though.   \n",
       "1046     53477   53477n  But in most situations in which a firm provide...   \n",
       "1047     92836   92836n                                     Go ahead Adam.   \n",
       "1048    117942  117942c  They could not recall that any of the passenge...   \n",
       "1049    120094  120094n  Others who worked with ACM's high school progr...   \n",
       "\n",
       "                                   premise_binary_parse  \\\n",
       "0     ( ( Your contribution ) ( ( helped ( make ( it...   \n",
       "1     ( ( ( ( ( ( The answer ) ( ( ( ( has nothing )...   \n",
       "2     ( We ( ( serve ( ( a ( classic ( Tuscan meal )...   \n",
       "3     ( ( ( A ( few months ) ) ago ) ( , ( ( ( ( Car...   \n",
       "4     ( I ( ( was ( on ( ( this earth ) ( you ( know...   \n",
       "...                                                 ...   \n",
       "1045  ( That ( ( ( ( 's ( ( a ( little bit ) ) ( of ...   \n",
       "1046  ( But ( ( in ( ( most situations ) ( ( in whic...   \n",
       "1047                        ( ( ( Go ahead ) Adam ) . )   \n",
       "1048  ( They ( ( ( could not ) ( recall ( that ( ( a...   \n",
       "1049  ( ( Others ( who ( ( ( ( ( worked ( with ( ( A...   \n",
       "\n",
       "                                          premise_parse  \\\n",
       "0     (ROOT (S (NP (PRP$ Your) (NN contribution)) (V...   \n",
       "1     (ROOT (S (S (NP (DT The) (NN answer)) (VP (VBZ...   \n",
       "2     (ROOT (S (NP (PRP We)) (VP (VBP serve) (NP (NP...   \n",
       "3     (ROOT (S (ADVP (NP (DT A) (JJ few) (NNS months...   \n",
       "4     (ROOT (S (NP (PRP I)) (VP (VBD was) (PP (IN on...   \n",
       "...                                                 ...   \n",
       "1045  (ROOT (S (NP (DT That)) (VP (VBZ 's) (NP (NP (...   \n",
       "1046  (ROOT (S (CC But) (PP (IN in) (NP (NP (JJS mos...   \n",
       "1047  (ROOT (S (VP (VB Go) (PRT (RP ahead)) (NP (NNP...   \n",
       "1048  (ROOT (S (NP (PRP They)) (VP (MD could) (RB no...   \n",
       "1049  (ROOT (S (NP (NP (NNS Others)) (SBAR (WHNP (WP...   \n",
       "\n",
       "                                             hypothesis  \\\n",
       "0     Your contributions were of no help with our st...   \n",
       "1     Dictionaries are indeed exercises in bi-unique...   \n",
       "2                We serve a meal of Florentine terrine.   \n",
       "3     Carl Newton and I have never had any other pre...   \n",
       "4     I don't yet know the reason why I have lived o...   \n",
       "...                                                 ...   \n",
       "1045                That is a small amount of pressure.   \n",
       "1046  The high demand variation products are typical...   \n",
       "1047                   You can win the tournament Adam.   \n",
       "1048  They did not screen any passengers, and had no...   \n",
       "1049  These traditions are important to school culture.   \n",
       "\n",
       "                                hypothesis_binary_parse  \\\n",
       "0     ( ( Your contributions ) ( ( were ( of ( ( no ...   \n",
       "1     ( Dictionaries ( ( ( are indeed ) ( exercises ...   \n",
       "2     ( We ( ( serve ( ( a meal ) ( of ( Florentine ...   \n",
       "3     ( ( ( ( Carl Newton ) and ) I ) ( ( ( have nev...   \n",
       "4     ( I ( ( ( ( do n't ) yet ) ( ( know ( the reas...   \n",
       "...                                                 ...   \n",
       "1045  ( That ( ( is ( ( a ( small amount ) ) ( of pr...   \n",
       "1046  ( ( The ( high ( demand ( variation products )...   \n",
       "1047  ( You ( ( can ( win ( the ( tournament Adam ) ...   \n",
       "1048  ( They ( ( ( ( ( ( did not ) ( screen ( any pa...   \n",
       "1049  ( ( These traditions ) ( ( are ( important ( t...   \n",
       "\n",
       "                                       hypothesis_parse       genre  label  \n",
       "0     (ROOT (S (NP (PRP$ Your) (NNS contributions)) ...     letters      2  \n",
       "1     (ROOT (S (NP (NNS Dictionaries)) (VP (VBP are)...    verbatim      2  \n",
       "2     (ROOT (S (NP (PRP We)) (VP (VBP serve) (NP (NP...    verbatim      0  \n",
       "3     (ROOT (S (NP (NP (NNP Carl) (NNP Newton)) (CC ...     letters      2  \n",
       "4     (ROOT (S (NP (PRP I)) (VP (VBP do) (RB n't) (A...  facetoface      0  \n",
       "...                                                 ...         ...    ...  \n",
       "1045  (ROOT (S (NP (DT That)) (VP (VBZ is) (NP (NP (...  facetoface      0  \n",
       "1046  (ROOT (S (NP (DT The) (JJ high) (NN demand) (N...         oup      1  \n",
       "1047  (ROOT (S (NP (PRP You)) (VP (MD can) (VP (VB w...  facetoface      1  \n",
       "1048  (ROOT (S (NP (PRP They)) (VP (VP (VBD did) (RB...  nineeleven      2  \n",
       "1049  (ROOT (S (NP (DT These) (NNS traditions)) (VP ...     letters      1  \n",
       "\n",
       "[1050 rows x 10 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T20:07:11.320103Z",
     "start_time": "2024-02-03T20:07:10.895666900Z"
    },
    "id": "Gk96lNh94_e_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pickle\n",
    "import os\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class MNLIDataBert(Dataset):\n",
    "\n",
    "  def __init__(self, train_df, val_df):\n",
    "    #self.label_dict = {'entailment': 0, 'contradiction': 1, 'neutral': 2}\n",
    "\n",
    "    self.train_df = train_df\n",
    "    self.val_df = val_df\n",
    "\n",
    "    self.base_path = '/content/'\n",
    "    self.tokenizer = BertTokenizer.from_pretrained('bert_weights_speedtest', do_lower_case=True)\n",
    "    self.train_data = None\n",
    "    self.val_data = None\n",
    "    self.init_data()\n",
    "\n",
    "  def init_data(self):\n",
    "    # Saving takes too much RAM\n",
    "    #\n",
    "    # if os.path.exists(os.path.join(self.base_path, 'train_data.pkl')):\n",
    "    #   print(\"Found training data\")\n",
    "    #   with open(os.path.join(self.base_path, 'train_data.pkl'), 'rb') as f:\n",
    "    #     self.train_data = pickle.load(f)\n",
    "    # else:\n",
    "    #   self.train_data = self.load_data(self.train_df)\n",
    "    #   with open(os.path.join(self.base_path, 'train_data.pkl'), 'wb') as f:\n",
    "    #     pickle.dump(self.train_data, f)\n",
    "    # if os.path.exists(os.path.join(self.base_path, 'val_data.pkl')):\n",
    "    #   print(\"Found val data\")\n",
    "    #   with open(os.path.join(self.base_path, 'val_data.pkl'), 'rb') as f:\n",
    "    #     self.val_data = pickle.load(f)\n",
    "    # else:\n",
    "    #   self.val_data = self.load_data(self.val_df)\n",
    "    #   with open(os.path.join(self.base_path, 'val_data.pkl'), 'wb') as f:\n",
    "    #     pickle.dump(self.val_data, f)\n",
    "    self.train_data = self.load_data(self.train_df)\n",
    "    self.val_data = self.load_data(self.val_df)\n",
    "\n",
    "  def load_data(self, df):\n",
    "    MAX_LEN = 512\n",
    "    token_ids = []\n",
    "    mask_ids = []\n",
    "    seg_ids = []\n",
    "    y = []\n",
    "\n",
    "    premise_list = df['premise'].to_list()\n",
    "    hypothesis_list = df['hypothesis'].to_list()\n",
    "    label_list = df['label'].to_list()\n",
    "    #print(label_list)\n",
    "\n",
    "    for (_premise, _hypothesis, _label) in zip(premise_list, hypothesis_list, label_list):\n",
    "      premise_id = self.tokenizer.encode(_premise, add_special_tokens = False)\n",
    "      hypothesis_id = self.tokenizer.encode(_hypothesis, add_special_tokens = False)\n",
    "      pair_token_ids = [self.tokenizer.cls_token_id] + premise_id + [self.tokenizer.sep_token_id] + hypothesis_id + [self.tokenizer.sep_token_id]\n",
    "      premise_len = len(premise_id)\n",
    "      hypothesis_len = len(hypothesis_id)\n",
    "\n",
    "      segment_ids = torch.tensor([0] * (premise_len + 2) + [1] * (hypothesis_len + 1))  # sentence 0 and sentence 1\n",
    "      attention_mask_ids = torch.tensor([1] * (premise_len + hypothesis_len + 3))  # mask padded values\n",
    "\n",
    "      token_ids.append(torch.tensor(pair_token_ids))\n",
    "      seg_ids.append(segment_ids)\n",
    "      mask_ids.append(attention_mask_ids)\n",
    "      # print(_label)\n",
    "      # print(self.label_dict.keys())\n",
    "      y.append(_label)\n",
    "    \n",
    "    token_ids = pad_sequence(token_ids, batch_first=True)\n",
    "    mask_ids = pad_sequence(mask_ids, batch_first=True)\n",
    "    seg_ids = pad_sequence(seg_ids, batch_first=True)\n",
    "    y = torch.tensor(y)\n",
    "    dataset = TensorDataset(token_ids, mask_ids, seg_ids, y)\n",
    "    print(len(dataset))\n",
    "    return dataset\n",
    "\n",
    "  def get_data_loaders(self, batch_size=32, shuffle=True):\n",
    "    train_loader = DataLoader(\n",
    "      self.train_data,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "      self.val_data,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T20:07:16.603894500Z",
     "start_time": "2024-02-03T20:07:11.320103Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "md52P1z14_e_",
    "outputId": "3421aec1-128f-4589-874d-a82372138365"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3500\n",
      "1050\n"
     ]
    }
   ],
   "source": [
    "mnli_dataset = MNLIDataBert(train_df, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T20:07:16.614894800Z",
     "start_time": "2024-02-03T20:07:16.603894500Z"
    },
    "id": "-LSyABLG4_fA"
   },
   "outputs": [],
   "source": [
    "train_loader, val_loader = mnli_dataset.get_data_loaders(batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T20:07:17.849848700Z",
     "start_time": "2024-02-03T20:07:16.614894800Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qOdc4Cs2DEjt",
    "outputId": "98759f4f-ead1-4b1d-c20c-0ce4996de3ed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert_weights_speedtest were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert_weights_speedtest and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert_weights_speedtest\", num_labels=3)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T20:07:17.850815Z",
     "start_time": "2024-02-03T20:07:17.834812400Z"
    },
    "id": "jxzpENXlEh-u"
   },
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T20:07:17.850815Z",
     "start_time": "2024-02-03T20:07:17.834812400Z"
    },
    "id": "is1TqwTREid9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haehnel/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# This variable contains all of the hyperparemeter information our training loop needs\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, correct_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T20:07:17.914836300Z",
     "start_time": "2024-02-03T20:07:17.835837800Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "79aI1dun4_fB",
    "outputId": "c2f15bac-22d1-403b-cc1b-0a758cae0433"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 109,484,547 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T20:07:17.915843300Z",
     "start_time": "2024-02-03T20:07:17.866812500Z"
    },
    "id": "qhU855Cw4_fB"
   },
   "outputs": [],
   "source": [
    "def multi_acc(y_pred, y_test):\n",
    "  acc = (torch.log_softmax(y_pred, dim=1).argmax(dim=1) == y_test).sum().float() / float(y_test.size(0))\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T20:07:17.952813100Z",
     "start_time": "2024-02-03T20:07:17.878840800Z"
    },
    "id": "OfhYO7Db4_fB"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer):  \n",
    "  total_step = len(train_loader)\n",
    "\n",
    "  for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_train_acc  = 0\n",
    "    for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in enumerate(train_loader):\n",
    "      optimizer.zero_grad()\n",
    "      pair_token_ids = pair_token_ids.to(device)\n",
    "      mask_ids = mask_ids.to(device)\n",
    "      seg_ids = seg_ids.to(device)\n",
    "      labels = y.to(device)\n",
    "      # prediction = model(pair_token_ids, mask_ids, seg_ids)\n",
    "      loss, prediction = model(pair_token_ids, \n",
    "                             token_type_ids=seg_ids, \n",
    "                             attention_mask=mask_ids, \n",
    "                             labels=labels).values()\n",
    "\n",
    "      # loss = criterion(prediction, labels)\n",
    "      acc = multi_acc(prediction, labels)\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      \n",
    "      total_train_loss += loss.item()\n",
    "      total_train_acc  += acc.item()\n",
    "\n",
    "    train_acc  = total_train_acc/len(train_loader)\n",
    "    train_loss = total_train_loss/len(train_loader)\n",
    "    model.eval()\n",
    "    total_val_acc  = 0\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "      for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in enumerate(val_loader):\n",
    "        optimizer.zero_grad()\n",
    "        pair_token_ids = pair_token_ids.to(device)\n",
    "        mask_ids = mask_ids.to(device)\n",
    "        seg_ids = seg_ids.to(device)\n",
    "        labels = y.to(device)\n",
    "\n",
    "        # prediction = model(pair_token_ids, mask_ids, seg_ids)\n",
    "        loss, prediction = model(pair_token_ids, \n",
    "                             token_type_ids=seg_ids, \n",
    "                             attention_mask=mask_ids, \n",
    "                             labels=labels).values()\n",
    "        \n",
    "        # loss = criterion(prediction, labels)\n",
    "        acc = multi_acc(prediction, labels)\n",
    "\n",
    "        total_val_loss += loss.item()\n",
    "        total_val_acc  += acc.item()\n",
    "\n",
    "    val_acc  = total_val_acc/len(val_loader)\n",
    "    val_loss = total_val_loss/len(val_loader)\n",
    "    end = time.time()\n",
    "    hours, rem = divmod(end-start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "    print(f'Epoch {epoch+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
    "    print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T20:07:34.955503600Z",
     "start_time": "2024-02-03T20:07:17.927812600Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GPIxT4RB4_fC",
    "outputId": "4909515f-1869-45da-cf0e-8491493067b9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "train(model, train_loader, val_loader, optimizer)\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Fine-tuning took {elapsed_time/3600} hours.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T20:07:34.960570100Z",
     "start_time": "2024-02-03T20:07:34.957502400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Sentence Entailment BERT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
